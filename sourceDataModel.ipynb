{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> SourceDataModel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> in database in tabel Sales_salesterritory attributt 'Group' veranderd naar 'Group1' vanwege error in code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import warnings\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "print(pyodbc.drivers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> inlezen database: AdventureWorks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing the CSV files\n",
    "csv_folder = \"databases/AdventureWorks\"\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(csv_folder) if f.endswith('.csv')]\n",
    "print(f\"Found CSV files: {csv_files}\")\n",
    "\n",
    "# Initialize a dictionary to store DataFrames\n",
    "df_AdventureWorks = {}\n",
    "\n",
    "# Loop through each CSV file and load it into a DataFrame\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(csv_folder, csv_file)\n",
    "    print(f\"Loading file: {file_path}\")\n",
    "    try:\n",
    "        # Try reading the file with a fallback encoding\n",
    "        df = pd.read_csv(file_path, encoding='latin1')  # Use 'latin1' or 'iso-8859-1' if UTF-8 fails\n",
    "        # Store the DataFrame in the dictionary with the table name as the key\n",
    "        table_name = os.path.splitext(csv_file)[0]\n",
    "        df_AdventureWorks[table_name] = df\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {csv_file}: {e}\")\n",
    "\n",
    "# Access individual DataFrames by their table name\n",
    "for table_name, df in df_AdventureWorks.items():\n",
    "    print(f\"Table: {table_name}, Rows: {len(df)}\\n{df.head()}\\n\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "\n",
    "#function to clean nan values\n",
    "def clean_nan_values(dw):\n",
    "    for table_name, df in dw.items():\n",
    "        # Zet alle NaN naar None zodat SQL Server NULL kan verwerken\n",
    "        dw[table_name] = df.astype(object).where(pd.notnull(df), None)\n",
    "    return dw\n",
    "\n",
    "# Clean NaN values\n",
    "df_AdventureWorks = clean_nan_values(df_AdventureWorks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> inlezen database: aenc.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the SQLite database\n",
    "sqlite_file = \"databases/aenc.sqlite\"\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "# Get a list of all tables in the database\n",
    "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = pd.read_sql(query, conn)\n",
    "table_names = tables['name'].tolist()\n",
    "print(f\"Found tables: {table_names}\")\n",
    "\n",
    "# Initialize a dictionary to store DataFrames\n",
    "df_aenc = {}\n",
    "\n",
    "# Loop through each table and load it into a DataFrame\n",
    "for table_name in table_names:\n",
    "    print(f\"Loading table: {table_name}\")\n",
    "    try:\n",
    "        # Read the table into a DataFrame\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table_name};\", conn)\n",
    "        # Store the DataFrame in the dictionary with the table name as the key\n",
    "        df_aenc[table_name] = df\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load table {table_name}: {e}\")\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Access individual DataFrames by their table name\n",
    "for table_name, df in df_aenc.items():\n",
    "    print(f\"Table: {table_name}, Rows: {len(df)}\\n{df.head()}\\n\")\n",
    "\n",
    "\n",
    "# Clean NaN values in the DataFrames\n",
    "df_aenc = clean_nan_values(df_aenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Inlezen database: NorthWind SQL server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbindingsgegevens\n",
    "server = '127.0.0.1'        \n",
    "port = '1433'               \n",
    "database = 'NorthWind'         \n",
    "username = 'SA'             \n",
    "password = 'iDTyjZx7dRL4'  \n",
    "\n",
    "# Connection string\n",
    "connection_string = (\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server},{port};\"\n",
    "    f\"DATABASE={database};\"\n",
    "    f\"UID={username};\"\n",
    "    f\"PWD={password};\"\n",
    "    \"TrustServerCertificate=yes;\"\n",
    "    \"Timeout=30;\"\n",
    ")\n",
    "\n",
    "# Maak verbinding met de database\n",
    "conn = pyodbc.connect(connection_string)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Haal alle tabellen op\n",
    "cursor.execute(\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'\")\n",
    "tables = [row.TABLE_NAME for row in cursor.fetchall()]\n",
    "\n",
    "# Dictionary om alle dataframes op te slaan\n",
    "df_NorthWind = {}\n",
    "\n",
    "# Loop door alle tabellen en laad ze in Pandas DataFrames\n",
    "for table in tables:\n",
    "    query = f\"SELECT * FROM [{table}]\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df_NorthWind[table] = df\n",
    "    print(f\"Tabel '{table}' ingelezen met {df.shape[0]} rijen en {df.shape[1]} kolommen.\")\n",
    "\n",
    "# Sluit de verbinding\n",
    "conn.close()\n",
    "\n",
    "# Print de kolomnamen en de eerste paar rijen van elke DataFrame\n",
    "for table_name, df in df_NorthWind.items():\n",
    "    print(f\"\\nTable: {table_name}\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(df.head())\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>maak dictionary aan voor alle goede dataframes + mapping dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_sourcedatamodel = {}\n",
    "df_mapping = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: Production_ProductCategory\n",
    "<h4> bronnen: Production_ProductCategory + Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data opnieuw laden om dubbele kolommen te voorkomen\n",
    "df_product_category = df_AdventureWorks.get(\"Production_ProductCategory\").copy()\n",
    "df_categories = df_NorthWind.get(\"Categories\").copy()\n",
    "\n",
    "# Voeg 'Source' kolom toe\n",
    "df_product_category['Source'] = 'AdventureWorks'\n",
    "df_categories['Source'] = 'NorthWind'\n",
    "\n",
    "\n",
    "# Voeg een unieke sleutel toe (MergedCategoryID) als deze nog niet bestaat\n",
    "if \"MergedCategoryID\" not in df_product_category.columns:\n",
    "    df_product_category.insert(0, \"MergedCategoryID\", range(1, len(df_product_category) + 1))\n",
    "\n",
    "if \"MergedCategoryID\" not in df_categories.columns:\n",
    "    df_categories.insert(0, \"MergedCategoryID\", range(len(df_product_category) + 1, len(df_product_category) + len(df_categories) + 1))\n",
    "\n",
    "# Hernoem kolommen zodat ze overeenkomen\n",
    "df_categories.rename(columns={\n",
    "    \"CategoryID\": \"ProductCategoryID\", \n",
    "    \"CategoryName\": \"Name\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Combineer de twee tabellen onder elkaar\n",
    "merged_df_productCategorie = pd.concat([df_product_category, df_categories], ignore_index=True)\n",
    "\n",
    "# Vul NaN-waarden in met lege strings (optioneel)\n",
    "merged_df_productCategorie.fillna(\"\", inplace=True)\n",
    "\n",
    "# Verwijder de kolom ShipDate als deze bestaat\n",
    "if \"Picture\" in merged_df_productCategorie.columns:\n",
    "    merged_df_productCategorie = merged_df_productCategorie.drop(columns=[\"Picture\"])\n",
    "\n",
    "\n",
    "\n",
    "#mapping van de categorieen per bron\n",
    "category_mapping = merged_df_productCategorie[['ProductCategoryID', 'Source', 'MergedCategoryID']]\n",
    "\n",
    "dfs_sourcedatamodel[\"Production_ProductCategory\"] = merged_df_productCategorie\n",
    "df_mapping[\"category_mapping\"] = category_mapping\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print het resultaat\n",
    "print(merged_df_productCategorie)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: Production_Product\n",
    "<h4> bronnen: Production_Product + Products + Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functie om alle datums correct te converteren\n",
    "def convert_dates(df):\n",
    "    date_columns = [col for col in df.columns if \"date\" in col.lower()]\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')  # Zet om naar datetime\n",
    "    return df\n",
    "\n",
    "# Functie om -1 te vervangen door None\n",
    "def clean_negative_values(df):\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        df[col] = df[col].apply(lambda x: None if x == -1 else x)\n",
    "    return df\n",
    "\n",
    "# Data opnieuw laden om dubbele kolommen te voorkomen\n",
    "df_production_product = df_AdventureWorks.get(\"Production_Product\").copy()\n",
    "df_products = df_NorthWind.get(\"Products\").copy()\n",
    "df_product = df_aenc.get(\"Product\").copy()\n",
    "\n",
    "# Voeg 'Source' kolom toe\n",
    "df_production_product['Source'] = 'AdventureWorks'\n",
    "df_products['Source'] = 'NorthWind'\n",
    "df_product['Source'] = 'AENC'\n",
    "\n",
    "# Maak een unieke MergedID voor elke rij\n",
    "df_production_product.insert(0, \"MergedID\", range(1, len(df_production_product) + 1))\n",
    "df_products.insert(0, \"MergedID\", range(len(df_production_product) + 1, len(df_production_product) + len(df_products) + 1))\n",
    "df_product.insert(0, \"MergedID\", range(len(df_production_product) + len(df_products) + 1, \n",
    "                                       len(df_production_product) + len(df_products) + len(df_product) + 1))\n",
    "\n",
    "# Zorg dat alle tabellen dezelfde kolommen hebben\n",
    "df_products.rename(columns={\"ProductName\": \"Name\", \"CategoryID\": \"ProductCategoryID\", \"UnitPrice\": \"ListPrice\"}, inplace=True)\n",
    "df_product.rename(columns={\"id\": \"ProductID\", \"name\": \"Name\", \"prod_size\": \"Size\", \"unit_price\": \"ListPrice\", \"Category\": \"ProductCategoryID\"}, inplace=True)\n",
    "\n",
    "# Voeg ontbrekende kolommen toe met lege waarden\n",
    "all_columns = set(df_production_product.columns).union(set(df_products.columns)).union(set(df_product.columns))\n",
    "for df in [df_production_product, df_products, df_product]:\n",
    "    for col in all_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "# Zet alle tabellen onder elkaar\n",
    "merged_df_product = pd.concat([df_production_product, df_products, df_product], ignore_index=True)\n",
    "\n",
    "# Datumconversie toepassen\n",
    "merged_df_product = convert_dates(merged_df_product)\n",
    "\n",
    "# Negatieve waarden verwijderen\n",
    "merged_df_product = clean_negative_values(merged_df_product)\n",
    "\n",
    "# Verwijder 'picture_name' en 'DiscontinuedDate' kolommen\n",
    "merged_df_product.drop(columns=[\"picture_name\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Hernoem 'ReorderLevel' naar 'ReorderPoint' en zet ze samen in één kolom\n",
    "if \"ReorderLevel\" in merged_df_product.columns and \"ReorderPoint\" in merged_df_product.columns:\n",
    "    merged_df_product[\"ReorderPoint\"] = merged_df_product[\"ReorderPoint\"].combine_first(merged_df_product[\"ReorderLevel\"])\n",
    "    merged_df_product.drop(columns=[\"ReorderLevel\"], inplace=True)\n",
    "\n",
    "# Hernoem 'color' zodat beide versies samengevoegd worden in één kolom\n",
    "if \"color\" in merged_df_product.columns and \"Color\" in merged_df_product.columns:\n",
    "    merged_df_product[\"Color\"] = merged_df_product[\"Color\"].combine_first(merged_df_product[\"color\"])\n",
    "    merged_df_product.drop(columns=[\"color\"], inplace=True)\n",
    "\n",
    "product_mapping = merged_df_product[['ProductID', 'Source', 'MergedID']]\n",
    "\n",
    "# Merge op ProductCategoryID + Source om conflicten te vermijden\n",
    "merged_df_product = merged_df_product.merge(\n",
    "    category_mapping,\n",
    "    how='left',\n",
    "    left_on=['ProductCategoryID', 'Source'],\n",
    "    right_on=['ProductCategoryID', 'Source']\n",
    ")\n",
    "\n",
    "# Vervang de oude ProductCategoryID door de nieuwe MergedCategoryID\n",
    "merged_df_product['ProductCategoryID'] = merged_df_product['MergedCategoryID']\n",
    "merged_df_product.drop(columns=['MergedCategoryID'], inplace=True, errors='ignore')\n",
    "\n",
    "# Verwijder de kolom ShipDate als deze bestaat\n",
    "if \"DiscontinuedDate\" in merged_df_product.columns:\n",
    "    merged_df_product = merged_df_product.drop(columns=[\"DiscontinuedDate\"])\n",
    "\n",
    "# Data opslaan in het data warehouse dictionary\n",
    "dfs_sourcedatamodel[\"Production_Product\"] = merged_df_product\n",
    "df_mapping[\"product_mapping\"] = product_mapping\n",
    "\n",
    "# Print het resultaat\n",
    "print(merged_df_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: Purchasing_Vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get dataframe from dictionary\n",
    "purchasing_vendor_df = df_AdventureWorks.get(\"Purchasing_Vendor\")\n",
    "\n",
    "# Check if the DataFrame exists\n",
    "if purchasing_vendor_df is not None:\n",
    "    # Add the DataFrame to the sourcedatamodel dictionary\n",
    "    dfs_sourcedatamodel[\"Purchasing_Vendor\"] = purchasing_vendor_df\n",
    "\n",
    "    print(f\"Rows: {len(purchasing_vendor_df)}\")\n",
    "    print(purchasing_vendor_df.head())\n",
    "else:\n",
    "    print(\"The table 'Purchasing_Vendor' does not exist in database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: sales_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get dataframe from dictionary\n",
    "sales_store_df = df_AdventureWorks.get(\"Sales_Store\")\n",
    "\n",
    "# Check if the DataFrame exists\n",
    "if sales_store_df is not None:\n",
    "    # Add the DataFrame to the sourcedatamodel dictionary\n",
    "    dfs_sourcedatamodel[\"Sales_Store\"] = sales_store_df\n",
    "\n",
    "    print(f\"Rows: {len(sales_store_df)}\")\n",
    "    print(sales_store_df.head())\n",
    "else:\n",
    "    print(\"The table does not exist in database.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> tabel: employee_territories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get dataframe from dictionary\n",
    "employeeTerritories_df = df_NorthWind.get(\"EmployeeTerritories\")\n",
    "\n",
    "# Check if the DataFrame exists\n",
    "if employeeTerritories_df is not None:\n",
    "     # Apply the same offset as Employee_df2 for AENC employees\n",
    "    employeeTerritories_df[\"EmployeeID\"] = employeeTerritories_df[\"EmployeeID\"].astype(int) + 100000  \n",
    "    # Add the DataFrame to the sourcedatamodel dictionary\n",
    "    dfs_sourcedatamodel[\"EmployeeTerritories\"] = employeeTerritories_df\n",
    "\n",
    "    print(f\" Rows: {len(employeeTerritories_df)}\")\n",
    "    print(employeeTerritories_df.head())\n",
    "else:\n",
    "    print(\"The table does not exist in database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: Territories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dataframe from dictionary\n",
    "territories_df = df_NorthWind.get(\"Territories\")\n",
    "\n",
    "# Check if the DataFrame exists\n",
    "if territories_df is not None:\n",
    "    # Add the DataFrame to the sourcedatamodel dictionary\n",
    "    dfs_sourcedatamodel[\"Territories\"] = territories_df\n",
    "\n",
    "    print(f\" Rows: {len(territories_df)}\")\n",
    "    print(territories_df.head())\n",
    "else:\n",
    "    print(\"The table does not exist in database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dataframe from dictionary\n",
    "region_df = df_NorthWind.get(\"Region\")\n",
    "\n",
    "# Check if the DataFrame exists\n",
    "if region_df is not None:\n",
    "    # Add the DataFrame to the sourcedatamodel dictionary\n",
    "    dfs_sourcedatamodel[\"Region\"] = region_df\n",
    "\n",
    "    print(f\" Rows: {len(region_df)}\")\n",
    "    print(region_df.head())\n",
    "else:\n",
    "    print(\"The table does not exist in database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Table: Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframe from dictionary\n",
    "bonus_df = df_aenc.get(\"Bonus\")\n",
    "bonus_df = bonus_df.drop_duplicates(subset=[\"emp_id\", \"bonus_date\"])\n",
    "\n",
    "\n",
    "# Check if the DataFrame exists\n",
    "if bonus_df is not None:\n",
    "    # Apply the same offset as Employee_df2 for AENC employees\n",
    "    bonus_df[\"emp_id\"] = bonus_df[\"emp_id\"].astype(int) + 200000  \n",
    "\n",
    "    # Add the DataFrame to the sourcedatamodel dictionary\n",
    "    dfs_sourcedatamodel[\"Bonus\"] = bonus_df\n",
    "\n",
    "    print(f\"Rows: {len(bonus_df)}\")\n",
    "    print(bonus_df.head())\n",
    "else:\n",
    "    print(\"The table does not exist in database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: Sales_Customer\n",
    "\n",
    "<h4> Bronnen: Customer + Customers + Sales_Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dataframe from dictionary\n",
    "Sales_Customer_df = df_AdventureWorks.get(\"Sales_Customer\")\n",
    "\n",
    "\n",
    "# Laden van de datasets (vervang dit met je eigen DataFrame-imports)\n",
    "Sales_Customer_df = df_AdventureWorks.get(\"Sales_Customer\")\n",
    "customer_df = df_aenc.get(\"Customer\")   \n",
    "customers_df = df_NorthWind.get(\"Customers\")\n",
    "# Voeg 'Source' kolom toe\n",
    "Sales_Customer_df[\"Source\"] = \"AdventureWorks\"\n",
    "customer_df[\"Source\"] = \"AENC\"\n",
    "customers_df[\"Source\"] = \"NorthWind\"\n",
    "\n",
    "# Zorg dat CustomerID overal een string is om inconsistenties te voorkomen\n",
    "Sales_Customer_df[\"CustomerID\"] = Sales_Customer_df[\"CustomerID\"].astype(str)\n",
    "customers_df[\"CustomerID\"] = customers_df[\"CustomerID\"].astype(str)\n",
    "customer_df.rename(columns={\"id\": \"CustomerID\"}, inplace=True)\n",
    "customer_df[\"CustomerID\"] = customer_df[\"CustomerID\"].astype(str)\n",
    "\n",
    "# Hernoem kolommen om aan te sluiten bij Sales_Customer structuur\n",
    "customers_df.rename(columns={\"CompanyName\": \"CompanyName\"}, inplace=True)\n",
    "customer_df.rename(columns={\"company_name\": \"CompanyName\"}, inplace=True)\n",
    "\n",
    "# Houd alleen de relevante kolommen\n",
    "Sales_Customer_df = Sales_Customer_df[[\"CustomerID\", \"PersonID\", \"StoreID\", \"TerritoryID\", \"AccountNumber\", \"rowguid\", \"ModifiedDate\", \"Source\"]]\n",
    "customer_df = customer_df[[\"CustomerID\", \"CompanyName\", \"Source\"]]\n",
    "customers_df = customers_df[[\"CustomerID\", \"CompanyName\", \"Source\"]]\n",
    "\n",
    "# Voeg een unieke sleutel toe (MergedCustomerID)\n",
    "Sales_Customer_df.insert(0, \"MergedCustomerID\", range(1, len(Sales_Customer_df) + 1))\n",
    "customer_df.insert(0, \"MergedCustomerID\", range(len(Sales_Customer_df) + 1, len(Sales_Customer_df) + len(customer_df) + 1))\n",
    "customers_df.insert(0, \"MergedCustomerID\", range(len(Sales_Customer_df) + len(customer_df) + 1, len(Sales_Customer_df) + len(customer_df) + len(customers_df) + 1))\n",
    "\n",
    "# Combineer de tabellen onder elkaar\n",
    "merged_customers = pd.concat([Sales_Customer_df, customer_df, customers_df], ignore_index=True)\n",
    "\n",
    "# Vul NaN-waarden in met None (SQL-compatible)\n",
    "merged_customers = merged_customers.where(pd.notna(merged_customers), None)\n",
    "\n",
    "# Mapping maken voor toekomstige foreign key updates\n",
    "customer_mapping = merged_customers[[\"CustomerID\", \"Source\", \"MergedCustomerID\"]]\n",
    "\n",
    "\n",
    "\n",
    "dfs_sourcedatamodel[\"Sales_Customer\"] = merged_customers\n",
    "df_mapping[\"customer_mapping\"] = customer_mapping\n",
    "\n",
    "\n",
    "\n",
    "# Print het resultaat\n",
    "print(merged_customers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: Sales_SalesTerritory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dataframe from dictionary\n",
    "Sales_SalesTerritory_df = df_AdventureWorks.get(\"Sales_SalesTerritory\")\n",
    "Sales_SalesTerritory_df.rename(columns={\"Group\": \"Group1\"}, inplace=True)\n",
    "\n",
    "# Check if the DataFrame exists\n",
    "if Sales_SalesTerritory_df is not None:\n",
    "\n",
    "    # Add the DataFrame to the sourcedatamodel dictionary\n",
    "    dfs_sourcedatamodel[\"Sales_SalesTerritory\"] = Sales_SalesTerritory_df\n",
    "\n",
    "    print(f\" Rows: {len(Sales_SalesTerritory_df)}\")\n",
    "    print(Sales_SalesTerritory_df.head())\n",
    "else:\n",
    "    print(\"The table does not exist in database.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: Purchasing_PurchaseOrderHeader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get dataframe from dictionary\n",
    "Purchasing_PurchaseOrderHeader = df_AdventureWorks.get(\"Purchasing_PurchaseOrderHeader\")\n",
    "\n",
    "# Check if the DataFrame exists\n",
    "if Purchasing_PurchaseOrderHeader is not None:\n",
    "    # Add the DataFrame to the sourcedatamodel dictionary\n",
    "    dfs_sourcedatamodel[\"Purchasing_PurchaseOrderHeader\"] = Purchasing_PurchaseOrderHeader\n",
    "\n",
    "    print(f\" Rows: {len(Purchasing_PurchaseOrderHeader)}\")\n",
    "    print(Purchasing_PurchaseOrderHeader.head())\n",
    "else:\n",
    "    print(\"The table does not exist in database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: Purchasing_PurchaseOrderDetail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Purchasing_PurchaseOrderDetail DataFrame from the dictionary\n",
    "Purchasing_PurchaseOrderDetail = df_AdventureWorks.get(\"Purchasing_PurchaseOrderDetail\")\n",
    "\n",
    "# Check if the DataFrame exists\n",
    "if Purchasing_PurchaseOrderDetail is not None:\n",
    "    # Add the DataFrame to the sourcedatamodel dictionary\n",
    "    dfs_sourcedatamodel[\"Purchasing_PurchaseOrderDetail\"] = Purchasing_PurchaseOrderDetail\n",
    "\n",
    "    # Perform the merge to replace ProductID with MergedID\n",
    "    Purchasing_PurchaseOrderDetail = Purchasing_PurchaseOrderDetail.merge(\n",
    "        product_mapping,  # This contains ProductID, Source, and MergedID\n",
    "        how='left',\n",
    "        left_on=['ProductID'],  # Assuming ProductID exists in this table\n",
    "        right_on=['ProductID']\n",
    "    )\n",
    "\n",
    "    # Replace ProductID with MergedID\n",
    "    Purchasing_PurchaseOrderDetail['ProductID'] = Purchasing_PurchaseOrderDetail['MergedID']\n",
    "\n",
    "    # Drop the MergedID column as it's no longer needed\n",
    "    Purchasing_PurchaseOrderDetail.drop(columns=['MergedID'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Drop rows where Source is 'NorthWind'\n",
    "    if 'Source' in Purchasing_PurchaseOrderDetail.columns:\n",
    "        Purchasing_PurchaseOrderDetail = Purchasing_PurchaseOrderDetail[Purchasing_PurchaseOrderDetail['Source'] != 'NorthWind']\n",
    "        Purchasing_PurchaseOrderDetail.reset_index(drop=True, inplace=True)\n",
    "        print(\"Rows with Source == 'NorthWind' have been removed.\")\n",
    "    else:\n",
    "        print(\"The 'Source' column does not exist in Purchasing_PurchaseOrderDetail.\")\n",
    "\n",
    "    # Update the dictionary with the modified DataFrame\n",
    "    dfs_sourcedatamodel[\"Purchasing_PurchaseOrderDetail\"] = Purchasing_PurchaseOrderDetail\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Rows: {len(Purchasing_PurchaseOrderDetail)}\")\n",
    "    print(Purchasing_PurchaseOrderDetail.head())\n",
    "\n",
    "   \n",
    "    print(\"The updated table has been saved to 'Purchasing_PurchaseOrderDetail.csv'.\")\n",
    "else:\n",
    "    print(\"The table 'Purchasing_PurchaseOrderDetail' does not exist in the database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: Suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dataframe from dictionary\n",
    "Suppliers = df_NorthWind.get(\"Suppliers\")\n",
    "\n",
    "\n",
    "# Check if the DataFrame exists\n",
    "if Suppliers is not None:\n",
    "    # Add the DataFrame to the sourcedatamodel dictionary\n",
    "    dfs_sourcedatamodel[\"Suppliers\"] = Suppliers\n",
    "\n",
    "    print(f\" Rows: {len(Suppliers)}\")\n",
    "    print(Suppliers.head())\n",
    "else:\n",
    "    print(\"The table does not exist in database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: Person_Person\n",
    "<h4> Bronnen: Person_Person + Employees + Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Laden van de datasets (Vervang dit met je eigen DataFrame-imports)\n",
    "Person_Person_df = df_AdventureWorks.get(\"Person_Person\")\n",
    "Employees_df = df_NorthWind.get(\"Employees\")\n",
    "Employee_df = df_aenc.get(\"Employee\")\n",
    "\n",
    "# Voeg 'Source' kolom toe om de herkomst te behouden\n",
    "Person_Person_df[\"Source\"] = \"AdventureWorks\"\n",
    "Employees_df[\"Source\"] = \"Northwind\"\n",
    "Employee_df[\"Source\"] = \"aenc\"\n",
    "\n",
    "# Hernoem kolommen zodat ze consistent zijn met Person_Person\n",
    "Employees_df.rename(columns={\"EmployeeID\": \"BusinessEntityID\", \"HomePhone\": \"PhoneNumber\"}, inplace=True)\n",
    "Employee_df.rename(columns={\"emp_id\": \"BusinessEntityID\", \"phone\": \"PhoneNumber\"}, inplace=True)\n",
    "\n",
    "# Voeg een unieke sleutel toe (MergedBusinessEntityID) om overlaps te voorkomen\n",
    "Person_Person_df[\"MergedBusinessEntityID\"] = Person_Person_df[\"BusinessEntityID\"]\n",
    "Employees_df[\"MergedBusinessEntityID\"] = Employees_df[\"BusinessEntityID\"] + 100000  # Offset voor Northwind\n",
    "Employee_df[\"MergedBusinessEntityID\"] = Employee_df[\"BusinessEntityID\"] + 200000  # Offset voor AENC\n",
    "\n",
    "# Houd alleen de relevante kolommen\n",
    "Person_Person_df = Person_Person_df[[\"MergedBusinessEntityID\", \"BusinessEntityID\", \"PersonType\", \"NameStyle\", \"Title\", \"LastName\", \"Suffix\", \"EmailPromotion\", \"rowguid\",  \"ModifiedDate\", \"Source\"]]\n",
    "Employees_df = Employees_df[[\"MergedBusinessEntityID\", \"BusinessEntityID\", \"Title\", \"LastName\", \"PhoneNumber\", \"Source\"]]\n",
    "Employee_df = Employee_df[[\"MergedBusinessEntityID\", \"BusinessEntityID\", \"emp_fname\", \"emp_lname\", \"PhoneNumber\", \"Source\"]]\n",
    "\n",
    "# Hernoemen van kolommen zodat ze overeenkomen\n",
    "Employees_df.rename(columns={\"Title\": \"Title\", \"LastName\": \"LastName\"}, inplace=True)\n",
    "Employee_df.rename(columns={\"emp_fname\": \"Title\", \"emp_lname\": \"LastName\"}, inplace=True)\n",
    "\n",
    "# Samenvoegen van de tabellen zonder merge (alle data onder elkaar)\n",
    "merged_person = pd.concat([Person_Person_df, Employees_df, Employee_df], ignore_index=True)\n",
    "\n",
    "dfs_sourcedatamodel[\"Person_Person\"] = merged_person\n",
    "\n",
    "\n",
    "# Mapping maken voor toekomstige foreign key updates\n",
    "person_mapping = merged_person[[\"BusinessEntityID\", \"Source\", \"MergedBusinessEntityID\"]]\n",
    "df_mapping[\"person_mapping\"] = person_mapping\n",
    "\n",
    "# Print het resultaat\n",
    "print(merged_person)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: HumanResources_Department\n",
    "<h4> bronnen: HumanResources_Department + Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Laden van de datasets (vervang met echte DataFrame-imports)\n",
    "HumanResources_Department_df = df_AdventureWorks.get(\"HumanResources_Department\")\n",
    "Department_df = df_aenc.get(\"Department\")\n",
    "\n",
    "# Voeg 'Source' kolom toe\n",
    "HumanResources_Department_df[\"Source\"] = \"AdventureWorks\"\n",
    "Department_df[\"Source\"] = \"AENC\"\n",
    "\n",
    "# Hernoem kolommen zodat ze consistent zijn\n",
    "Department_df.rename(columns={\"dept_id\": \"DepartmentID\", \"dept_name\": \"Name\"}, inplace=True)\n",
    "\n",
    "# Houd alleen de relevante kolommen\n",
    "HumanResources_Department_df = HumanResources_Department_df[[\"DepartmentID\", \"Name\", \"GroupName\", \"ModifiedDate\", \"Source\"]]\n",
    "Department_df = Department_df[[\"DepartmentID\", \"Name\", \"Source\"]]\n",
    "\n",
    "# Voeg een placeholder toe voor ontbrekende GroupName en ModifiedDate in AENC-data\n",
    "Department_df[\"GroupName\"] = None\n",
    "Department_df[\"ModifiedDate\"] = None\n",
    "\n",
    "# Samenvoegen van de tabellen zonder merge (alle data onder elkaar)\n",
    "merged_departments = pd.concat([HumanResources_Department_df, Department_df], ignore_index=True)\n",
    "\n",
    "dfs_sourcedatamodel[\"HumanResources_Department\"] = merged_departments\n",
    "\n",
    "print(HumanResources_Department_df[\"GroupName\"].str.len().max())  # Check maximale lengte \n",
    "\n",
    "# Print het resultaat\n",
    "print(merged_departments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: HumanResources_Employee\n",
    "<h4> Bronnen: HumanResources_Employee + Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Laden van de datasets (Vervang dit met je eigen DataFrame-imports)\n",
    "Person_Person_df = df_AdventureWorks.get(\"Person_Person\")\n",
    "Employees_df2 = df_NorthWind.get(\"Employees\")\n",
    "Employee_df2 = df_aenc.get(\"Employee\")\n",
    "HumanResources_Employee_df = df_AdventureWorks.get(\"HumanResources_Employee\")  # Nieuwe tabel\n",
    "\n",
    "# Voeg 'Source' kolom toe om de herkomst te behouden\n",
    "Person_Person_df[\"Source\"] = \"AdventureWorks\"\n",
    "Employees_df2[\"Source\"] = \"NorthWind\"\n",
    "Employee_df2[\"Source\"] = \"AENC\"\n",
    "\n",
    "# Hernoem kolommen zodat ze consistent zijn met Person_Person\n",
    "Employees_df2.rename(columns={\"EmployeeID\": \"BusinessEntityID\", \"Title\": \"JobTitle\", \"BirthDate\": \"BirthDate\", \"HireDate\": \"HireDate\", \"TitleOfCourtesy\": \"Gender\"}, inplace=True)\n",
    "Employee_df2.rename(columns={\"emp_id\": \"BusinessEntityID\", \"start_date\": \"HireDate\", \"birth_date\": \"BirthDate\", \"sex\": \"Gender\", \"salary\": \"salary\", \"manager_id\": \"ManagerID\",\"dept_id\":\"DepartmentID\"}, inplace=True)\n",
    "\n",
    "# Voeg een unieke sleutel toe (MergedBusinessEntityID) om overlaps te voorkomen\n",
    "Person_Person_df[\"MergedBusinessEntityID\"] = Person_Person_df[\"BusinessEntityID\"]\n",
    "Employees_df2[\"MergedBusinessEntityID\"] = Employees_df2[\"BusinessEntityID\"] + 100000  # Offset voor Northwind\n",
    "Employee_df2[\"MergedBusinessEntityID\"] = Employee_df2[\"BusinessEntityID\"] + 200000  # Offset voor AENC\n",
    "\n",
    "# Voeg lege waarden toe voor ontbrekende kolommen\n",
    "Employee_df2[\"JobTitle\"] = None\n",
    "\n",
    "# Mapping maken voor ManagerID verwijzing\n",
    "manager_mapping = Employee_df2[[\"BusinessEntityID\", \"MergedBusinessEntityID\"]].copy()\n",
    "manager_mapping.rename(columns={\"BusinessEntityID\": \"OldManagerID\", \"MergedBusinessEntityID\": \"NewManagerID\"}, inplace=True)\n",
    "\n",
    "# Houd alleen de relevante kolommen\n",
    "Person_Person_df = Person_Person_df[[\"MergedBusinessEntityID\", \"BusinessEntityID\", \"Source\"]]\n",
    "Employees_df2 = Employees_df2[[\"MergedBusinessEntityID\", \"BusinessEntityID\", \"JobTitle\", \"BirthDate\", \"HireDate\", \"Gender\", \"Source\"]]\n",
    "Employee_df2 = Employee_df2[[\"MergedBusinessEntityID\", \"BusinessEntityID\", \"JobTitle\", \"BirthDate\", \"HireDate\", \"Gender\", \"salary\", \"ManagerID\", \"Source\", \"DepartmentID\"]]\n",
    "\n",
    "# Merge managerID correct op basis van de mapping\n",
    "Employee_df2 = Employee_df2.merge(manager_mapping, how=\"left\", left_on=\"ManagerID\", right_on=\"OldManagerID\")\n",
    "Employee_df2.drop(columns=[\"OldManagerID\", \"ManagerID\"], inplace=True)\n",
    "Employee_df2.rename(columns={\"NewManagerID\": \"ManagerID\"}, inplace=True)\n",
    "\n",
    "# Merge Person_Person met HumanResources_Employee op BusinessEntityID\n",
    "Person_Person_df = Person_Person_df.merge(\n",
    "    HumanResources_Employee_df[[\"BusinessEntityID\", \"DepartmentID\", \"BirthDate\", \"HireDate\", \"JobTitle\", \"Gender\"]],\n",
    "    on=\"BusinessEntityID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Samenvoegen van de tabellen zonder concat (één enkele tabel)\n",
    "merged_human_resources_Employee = pd.concat([Person_Person_df, Employees_df2, Employee_df2], ignore_index=True)\n",
    "\n",
    "# Pas de volgorde van de kolommen aan\n",
    "column_order = [\"MergedBusinessEntityID\", \"BusinessEntityID\", \"JobTitle\", \"BirthDate\", \"HireDate\", \"Gender\", \"salary\", \"ManagerID\", \"DepartmentID\", \"Source\"]\n",
    "merged_human_resources_Employee = merged_human_resources_Employee[column_order]\n",
    "\n",
    "dfs_sourcedatamodel[\"HumanResources_Employee\"] = merged_human_resources_Employee\n",
    "\n",
    "# Mapping maken voor toekomstige foreign key updates\n",
    "employee_mapping = merged_human_resources_Employee[[\"BusinessEntityID\", \"Source\", \"MergedBusinessEntityID\"]]\n",
    "df_mapping[\"employee_mapping\"] = employee_mapping\n",
    "\n",
    "# Print het resultaat\n",
    "print(merged_human_resources_Employee)\n",
    "\n",
    "merged_human_resources_Employee[\"salary\"] = merged_human_resources_Employee[\"salary\"].fillna(0).astype(float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Table: Person_Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dataframe from dictionary\n",
    "Person_Adress_Df = df_AdventureWorks.get(\"Person_Address\")\n",
    "column_order = [\"AddressID\", \"AddressLine1\", \"AddressLine2\", \"City\", \"StateProvinceID\", \"PostalCode\",\"BusinessEntityID\"]\n",
    "Person_Adress_Df = Person_Adress_Df[column_order]\n",
    "\n",
    "# Check if the DataFrame exists\n",
    "if Person_Adress_Df is not None:\n",
    "    # Add the DataFrame to the sourcedatamodel dictionary\n",
    "    dfs_sourcedatamodel[\"Person_Address\"] = Person_Adress_Df\n",
    "\n",
    "    print(f\"Rows: {len(Person_Adress_Df)}\")\n",
    "    print(Person_Adress_Df.head())\n",
    "else:\n",
    "    print(\"The table does not exist in database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: Shippers\n",
    "<h4> bronnen: Shippers + Orders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dataframe from dictionary\n",
    "Shippers = df_NorthWind.get(\"Shippers\")\n",
    "\n",
    "# Check if the DataFrame exists\n",
    "if Shippers is not None:\n",
    "    # Add the DataFrame to the sourcedatamodel dictionary\n",
    "    dfs_sourcedatamodel[\"Shippers\"] = Shippers\n",
    "\n",
    "    print(f\" Rows: {len(Shippers)}\")\n",
    "    print(Shippers.head())\n",
    "else:\n",
    "    print(\"The table does not exist in database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> table: Sales_SalesOrderHeader\n",
    "<h4> Bronnen: Sales_SalesOrderHeader + Orders + Sales_Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== 1. Laad de datasets ==========\n",
    "sales_adventureworks_df = df_AdventureWorks.get(\"Sales_SalesOrderHeader\")\n",
    "northwind_orders_df = df_NorthWind.get(\"Orders\")\n",
    "aenc_sales_order_df = df_aenc.get(\"Sales_Order\")\n",
    "\n",
    "# ========== 2. Voeg Source toe aan elke DataFrame ==========\n",
    "sales_adventureworks_df[\"Source\"] = \"AdventureWorks\"\n",
    "northwind_orders_df[\"Source\"] = \"NorthWind\"\n",
    "aenc_sales_order_df[\"Source\"] = \"AENC\"\n",
    "\n",
    "# ========== 3. Hernoem kolommen voor consistentie ==========\n",
    "northwind_orders_df.rename(columns={\n",
    "    \"OrderID\": \"SalesOrderID\",\n",
    "    \"OrderDate\": \"OrderDate\",\n",
    "    \"ShippedDate\": \"ShipDate\",\n",
    "    \"CustomerID\": \"CustomerID\",\n",
    "    \"EmployeeID\": \"SalesPersonID\",\n",
    "    \"Freight\": \"SubTotal\",\n",
    "    \"ShipVia\": \"ShipVia\",\n",
    "    \"ShipCountry\": \"ShipCountry\",\n",
    "    \"ShipRegion\": \"ShipRegion\",\n",
    "    \"ShipCity\": \"ShipCity\"\n",
    "}, inplace=True)\n",
    "\n",
    "aenc_sales_order_df.rename(columns={\n",
    "    \"id\": \"SalesOrderID\",\n",
    "    \"order_date\": \"OrderDate\",\n",
    "    \"cust_id\": \"CustomerID\",\n",
    "    \"sales_rep\": \"SalesPersonID\",\n",
    "    \"region\": \"ShipRegion\"\n",
    "}, inplace=True)\n",
    "\n",
    "# ========== 4. Zorg dat CustomerID overal een string is ==========\n",
    "sales_adventureworks_df[\"CustomerID\"] = sales_adventureworks_df[\"CustomerID\"].astype(str)\n",
    "northwind_orders_df[\"CustomerID\"] = northwind_orders_df[\"CustomerID\"].astype(str)\n",
    "aenc_sales_order_df[\"CustomerID\"] = aenc_sales_order_df[\"CustomerID\"].astype(str)\n",
    "\n",
    "# Ook in de customer_mapping DataFrame\n",
    "customer_mapping[\"CustomerID\"] = customer_mapping[\"CustomerID\"].astype(str)\n",
    "\n",
    "# ========== 5. Ontbrekende kolommen aanvullen met None ==========\n",
    "for df in [sales_adventureworks_df, northwind_orders_df, aenc_sales_order_df]:\n",
    "    for col in [\"RevisionNumber\", \"TerritoryID\", \"SubTotal\", \"DueDate\",\n",
    "                \"ShipDate\", \"ShipVia\", \"ShipCountry\", \"ShipRegion\", \"ShipCity\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "# ========== 6. Selecteer en orden de relevante kolommen ==========\n",
    "cols = [\n",
    "    \"SalesOrderID\", \"RevisionNumber\", \"OrderDate\", \"DueDate\", \"ShipDate\", \n",
    "    \"CustomerID\", \"SalesPersonID\", \"TerritoryID\", \"SubTotal\", \n",
    "    \"ShipVia\", \"ShipCountry\", \"ShipRegion\", \"ShipCity\", \"Source\"\n",
    "]\n",
    "\n",
    "sales_adventureworks_df = sales_adventureworks_df[cols]\n",
    "northwind_orders_df = northwind_orders_df[cols]\n",
    "aenc_sales_order_df = aenc_sales_order_df[cols]\n",
    "\n",
    "# ========== 7. Concateneer de drie DataFrames ==========\n",
    "merged_sales_salesOrderHeader = pd.concat(\n",
    "    [sales_adventureworks_df, northwind_orders_df, aenc_sales_order_df], \n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# ========== 8. CustomerID bijwerken via customer_mapping ==========\n",
    "# Hier nemen we aan dat 'customer_mapping' al bestaat, met kolommen [\"CustomerID\", \"Source\", \"MergedCustomerID\"].\n",
    "merged_sales_salesOrderHeader = merged_sales_salesOrderHeader.merge(\n",
    "    customer_mapping,\n",
    "    on=[\"CustomerID\", \"Source\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Vervang de CustomerID door MergedCustomerID\n",
    "merged_sales_salesOrderHeader[\"CustomerID\"] = merged_sales_salesOrderHeader[\"MergedCustomerID\"]\n",
    "merged_sales_salesOrderHeader.drop(columns=[\"MergedCustomerID\"], inplace=True)\n",
    "\n",
    "# ========== 9. SalesPersonID bijwerken via employee_mapping ==========\n",
    "# Hier nemen we aan dat 'employee_mapping' al bestaat, met kolommen [\"BusinessEntityID\", \"Source\", \"MergedBusinessEntityID\"].\n",
    "merged_sales_salesOrderHeader = merged_sales_salesOrderHeader.merge(\n",
    "    employee_mapping,\n",
    "    left_on=[\"SalesPersonID\", \"Source\"],\n",
    "    right_on=[\"BusinessEntityID\", \"Source\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_emp\")\n",
    ")\n",
    "\n",
    "# Vervang SalesPersonID door de gemapte MergedBusinessEntityID\n",
    "merged_sales_salesOrderHeader[\"SalesPersonID\"] = merged_sales_salesOrderHeader[\"MergedBusinessEntityID\"]\n",
    "merged_sales_salesOrderHeader.drop(columns=[\"BusinessEntityID\", \"MergedBusinessEntityID\"], inplace=True)\n",
    "\n",
    "# ========== 10. Sla op in het data model en exporteer ==========\n",
    "dfs_sourcedatamodel[\"Sales_SalesOrderHeader\"] = merged_sales_salesOrderHeader\n",
    "print(merged_sales_salesOrderHeader.head())\n",
    "\n",
    "# Exporteer naar CSV (optioneel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Table: Sales_SalesOrderDetail\n",
    "<h4> Bronnen: Sales_SalesOrderDetail + OrderDetails + sales_order_item + product(aenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- 1. Laad de datasets -----\n",
    "Sales_SalesOrderDetail = df_AdventureWorks.get(\"Sales_SalesOrderDetail\")\n",
    "Sales_Order_Item = df_aenc.get(\"Sales_Order_Item\")\n",
    "OrderDetails = df_NorthWind.get(\"OrderDetails\")\n",
    "df_product2 = df_aenc.get(\"Product\")  \n",
    "\n",
    "df_product2.rename(columns={\"id\": \"ProductID\"}, inplace=True)\n",
    "\n",
    "# ----- 2. Voeg Source toe aan elke DataFrame -----\n",
    "Sales_SalesOrderDetail[\"Source\"] = \"AdventureWorks\"\n",
    "Sales_Order_Item[\"Source\"] = \"AENC\"\n",
    "OrderDetails[\"Source\"] = \"NorthWind\"\n",
    "\n",
    "# ----- 3. Verwerk de AdventureWorks data -----\n",
    "# De AdventureWorks tabel bevat al de meeste gewenste kolommen\n",
    "Sales_SalesOrderDetail = Sales_SalesOrderDetail.copy()\n",
    "\n",
    "Sales_SalesOrderDetail = Sales_SalesOrderDetail[[\"SalesOrderID\", \"SalesOrderDetailID\", \"OrderQty\", \"ProductID\", \n",
    "                 \"UnitPrice\", \"UnitPriceDiscount\", \"LineTotal\", \"Source\"]]\n",
    "\n",
    "# ----- 4. Verwerk de AENC data -----\n",
    "Sales_Order_Item = Sales_Order_Item.copy()\n",
    "# Hernoem de kolommen: \n",
    "# id -> SalesOrderID, line_id -> SalesOrderDetailID, prod_id -> ProductID, quantity -> OrderQty\n",
    "Sales_Order_Item.rename(columns={\n",
    "    \"id\": \"SalesOrderID\",\n",
    "    \"line_id\": \"SalesOrderDetailID\",\n",
    "    \"prod_id\": \"ProductID\",\n",
    "    \"quantity\": \"OrderQty\"\n",
    "}, inplace=True)\n",
    "# Zorg dat OrderQty als string is\n",
    "# Voor AENC ontbreekt UnitPrice en UnitPriceDiscount:\n",
    "# Probeer de unit_price te halen uit een extra DataFrame (df_product2) uit de AENC-bron\n",
    "if  df_product2 is not None:\n",
    "    # Zorg dat in df_product2 ook de kolom \"ProductID\" voorkomt; dan mergen we de unit_price\n",
    "    Sales_Order_Item = Sales_Order_Item.merge(df_product2[[\"ProductID\", \"unit_price\"]], on=\"ProductID\", how=\"left\")\n",
    "    Sales_Order_Item.rename(columns={\"unit_price\": \"UnitPrice\"}, inplace=True)\n",
    "else:\n",
    "   print(\"The table does not exist in database.\")\n",
    "\n",
    "# ----- 5. Verwerk de NorthWind data -----\n",
    "OrderDetails = OrderDetails.copy()\n",
    "# Hernoem de kolommen: OrderID -> SalesOrderID, Quantity -> OrderQty, Discount -> UnitPriceDiscount\n",
    "OrderDetails.rename(columns={\n",
    "    \"OrderID\": \"SalesOrderID\",\n",
    "    \"Quantity\": \"OrderQty\",\n",
    "    \"Discount\": \"UnitPriceDiscount\"\n",
    "}, inplace=True)\n",
    "# Creëer een SalesOrderDetailID omdat deze kolom ontbreekt: gebruik per SalesOrderID een cumulatieve telling\n",
    "OrderDetails[\"SalesOrderDetailID\"] = OrderDetails.groupby(\"SalesOrderID\").cumcount() + 1\n",
    "\n",
    "OrderDetails[\"LineTotal\"] = None\n",
    "OrderDetails = OrderDetails[[\"SalesOrderID\", \"SalesOrderDetailID\", \"OrderQty\", \"ProductID\", \n",
    "               \"UnitPrice\", \"UnitPriceDiscount\", \"LineTotal\", \"Source\"]]\n",
    "\n",
    "# ----- 6. Combineer de drie bronnen -----\n",
    "merged_orderdetail = pd.concat([Sales_SalesOrderDetail, Sales_Order_Item, OrderDetails], ignore_index=True)\n",
    "\n",
    "# ----- 7. Werk de ProductID bij via product_mapping -----\n",
    "# Verwacht dat product_mapping een DataFrame is met kolommen: [\"ProductID\", \"Source\", \"MergedID\"]\n",
    "# Merge de mapping op ProductID en Source en vervang de originele ProductID door de MergedID\n",
    "merged_orderdetail = merged_orderdetail.merge(\n",
    "    product_mapping,\n",
    "    on=[\"ProductID\", \"Source\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "merged_orderdetail[\"ProductID\"] = merged_orderdetail[\"MergedID\"]\n",
    "merged_orderdetail.drop(columns=[\"MergedID\"], inplace=True)\n",
    "\n",
    "\n",
    "# ----- 1. Laad de datasets -----\n",
    "Sales_SalesOrderDetail = df_AdventureWorks.get(\"Sales_SalesOrderDetail\")\n",
    "Sales_Order_Item = df_aenc.get(\"Sales_Order_Item\")\n",
    "OrderDetails = df_NorthWind.get(\"OrderDetails\")\n",
    "df_product2 = df_aenc.get(\"Product\")  \n",
    "\n",
    "df_product2.rename(columns={\"id\": \"ProductID\"}, inplace=True)\n",
    "\n",
    "# ----- 2. Voeg Source toe aan elke DataFrame -----\n",
    "Sales_SalesOrderDetail[\"Source\"] = \"AdventureWorks\"\n",
    "Sales_Order_Item[\"Source\"] = \"AENC\"\n",
    "OrderDetails[\"Source\"] = \"NorthWind\"\n",
    "\n",
    "# ----- 3. Verwerk de AdventureWorks data -----\n",
    "# De AdventureWorks tabel bevat al de meeste gewenste kolommen\n",
    "Sales_SalesOrderDetail = Sales_SalesOrderDetail.copy()\n",
    "\n",
    "Sales_SalesOrderDetail = Sales_SalesOrderDetail[[\"SalesOrderID\", \"SalesOrderDetailID\", \"OrderQty\", \"ProductID\", \n",
    "                 \"UnitPrice\", \"UnitPriceDiscount\", \"LineTotal\", \"Source\"]]\n",
    "\n",
    "# ----- 4. Verwerk de AENC data -----\n",
    "Sales_Order_Item = Sales_Order_Item.copy()\n",
    "# Hernoem de kolommen: \n",
    "# id -> SalesOrderID, line_id -> SalesOrderDetailID, prod_id -> ProductID, quantity -> OrderQty\n",
    "Sales_Order_Item.rename(columns={\n",
    "    \"id\": \"SalesOrderID\",\n",
    "    \"line_id\": \"SalesOrderDetailID\",\n",
    "    \"prod_id\": \"ProductID\",\n",
    "    \"quantity\": \"OrderQty\"\n",
    "}, inplace=True)\n",
    "# Zorg dat OrderQty als string is\n",
    "# Voor AENC ontbreekt UnitPrice en UnitPriceDiscount:\n",
    "# Probeer de unit_price te halen uit een extra DataFrame (df_product2) uit de AENC-bron\n",
    "if  df_product2 is not None:\n",
    "    # Zorg dat in df_product2 ook de kolom \"ProductID\" voorkomt; dan mergen we de unit_price\n",
    "    Sales_Order_Item = Sales_Order_Item.merge(df_product2[[\"ProductID\", \"unit_price\"]], on=\"ProductID\", how=\"left\")\n",
    "    Sales_Order_Item.rename(columns={\"unit_price\": \"UnitPrice\"}, inplace=True)\n",
    "else:\n",
    "   print(\"The table does not exist in database.\")\n",
    "\n",
    "# ----- 5. Verwerk de NorthWind data -----\n",
    "OrderDetails = OrderDetails.copy()\n",
    "# Hernoem de kolommen: OrderID -> SalesOrderID, Quantity -> OrderQty, Discount -> UnitPriceDiscount\n",
    "OrderDetails.rename(columns={\n",
    "    \"OrderID\": \"SalesOrderID\",\n",
    "    \"Quantity\": \"OrderQty\",\n",
    "    \"Discount\": \"UnitPriceDiscount\"\n",
    "}, inplace=True)\n",
    "# Creëer een SalesOrderDetailID omdat deze kolom ontbreekt: gebruik per SalesOrderID een cumulatieve telling\n",
    "OrderDetails[\"SalesOrderDetailID\"] = OrderDetails.groupby(\"SalesOrderID\").cumcount() + 1\n",
    "\n",
    "OrderDetails[\"LineTotal\"] = None\n",
    "OrderDetails = OrderDetails[[\"SalesOrderID\", \"SalesOrderDetailID\", \"OrderQty\", \"ProductID\", \n",
    "               \"UnitPrice\", \"UnitPriceDiscount\", \"LineTotal\", \"Source\"]]\n",
    "\n",
    "# ----- 6. Combineer de drie bronnen -----\n",
    "merged_orderdetail = pd.concat([Sales_SalesOrderDetail, Sales_Order_Item, OrderDetails], ignore_index=True)\n",
    "\n",
    "# ----- 7. Werk de ProductID bij via product_mapping -----\n",
    "# Verwacht dat product_mapping een DataFrame is met kolommen: [\"ProductID\", \"Source\", \"MergedID\"]\n",
    "# Merge de mapping op ProductID en Source en vervang de originele ProductID door de MergedID\n",
    "merged_orderdetail = merged_orderdetail.merge(\n",
    "    product_mapping,\n",
    "    on=[\"ProductID\", \"Source\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "merged_orderdetail[\"ProductID\"] = merged_orderdetail[\"MergedID\"]\n",
    "merged_orderdetail.drop(columns=[\"MergedID\"], inplace=True)\n",
    "\n",
    "# Verwijder de kolom ShipDate als deze bestaat\n",
    "if \"ship_date\" in merged_orderdetail.columns:\n",
    "    merged_orderdetail = merged_orderdetail.drop(columns=[\"ship_date\"])\n",
    "\n",
    "# ----- 8. Sla het resultaat op in het datamodel -----\n",
    "dfs_sourcedatamodel[\"Sales_SalesOrderDetail\"] = merged_orderdetail\n",
    "\n",
    "# Print het resultaat\n",
    "print(merged_orderdetail.head())\n",
    "\n",
    "\n",
    "# ----- 8. Sla het resultaat op in het datamodel -----\n",
    "dfs_sourcedatamodel[\"Sales_SalesOrderDetail\"] = merged_orderdetail\n",
    "\n",
    "# Print het resultaat\n",
    "print(merged_orderdetail.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Vullen SDM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tabel in dfs_sourcedatamodel:\n",
    "    print(f\"{tabel}: {dfs_sourcedatamodel[tabel].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Connectie leggen SDM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbindingsgegevens\n",
    "server = '127.0.0.1'        \n",
    "port = '1433'               \n",
    "database2 = 'SDMProject'         \n",
    "username = 'SA'             \n",
    "password = 'iDTyjZx7dRL4'  \n",
    "\n",
    "# Connection string\n",
    "connection_string2 = (\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server},{port};\"\n",
    "    f\"DATABASE={database2};\"\n",
    "    f\"UID={username};\"\n",
    "    f\"PWD={password};\"\n",
    "    \"TrustServerCertificate=yes;\"\n",
    "    \"Timeout=30;\"\n",
    ")\n",
    "\n",
    "# Maak verbinding met de database\n",
    "conn = pyodbc.connect(connection_string)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Haal alle tabellen op``\n",
    "cursor.execute(\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'\")\n",
    "tables = [row.TABLE_NAME for row in cursor.fetchall()]\n",
    "\n",
    "\n",
    "# Sluit de verbinding\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Converteer datatypen met datums naar DATETIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_all_dates(dw):\n",
    "    date_keywords = [\"date\", \"time\"]  # Zoek naar deze woorden in kolomnamen\n",
    "    \n",
    "    for table_name, df in dw.items():\n",
    "        for col in df.columns:\n",
    "            if any(keyword in col.lower() for keyword in date_keywords):  # Check op 'date' of 'time'\n",
    "                try:\n",
    "                    df[col] = pd.to_datetime(df[col], errors='coerce')  # Converteer naar datetime\n",
    "                    print(f\"✅ Geconverteerd: {table_name}.{col}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Fout bij converteren van {col} in {table_name}: {e}\")\n",
    "    \n",
    "    return dw\n",
    "\n",
    "# Pas de functie toe op je DataFrame-collectie\n",
    "dfs_sourcedatamodel = convert_all_dates(dfs_sourcedatamodel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Vullen van database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nan_values(dw):\n",
    "    for table_name, df in dw.items():\n",
    "        # Zet alle NaN naar None zodat SQL Server NULL kan verwerken\n",
    "        dw[table_name] = df.astype(object).where(pd.notnull(df), None)\n",
    "    return dw\n",
    "\n",
    "# Pas toe op je dataWarehouse dictionary\n",
    "dfs_sourcedatamodel = clean_nan_values(dfs_sourcedatamodel)\n",
    "\n",
    "\n",
    "def upload_dataframes_to_sql(dw):\n",
    "    try:\n",
    "        with pyodbc.connect(connection_string2, autocommit=False) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.fast_executemany = True  # Maakt batch-inserts sneller\n",
    "            \n",
    "            print(\"⏳ Uitschakelen van FOREIGN KEY constraints...\")\n",
    "            cursor.execute(\"EXEC sp_MSforeachtable 'ALTER TABLE ? NOCHECK CONSTRAINT ALL'\")\n",
    "            conn.commit()\n",
    "            \n",
    "            # Loop over elke tabel\n",
    "            for table_name, df in dw.items():\n",
    "                print(f\"\\nBezig met uploaden van tabel: {table_name}...\")\n",
    "                \n",
    "                columns = ', '.join([f'[{col}]' for col in df.columns])\n",
    "                placeholders = ', '.join(['?'] * len(df.columns))\n",
    "                insert_query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "                \n",
    "                # Gebruik batch commit\n",
    "                batch_size = 1000\n",
    "                data_list = [tuple(row) for _, row in df.iterrows()]\n",
    "                \n",
    "                for i in range(0, len(data_list), batch_size):\n",
    "                    try:\n",
    "                        cursor.executemany(insert_query, data_list[i:i+batch_size])\n",
    "                        conn.commit()\n",
    "                        print(f\"  ✅ Batch {i//batch_size + 1} geüpload ({len(data_list[i:i+batch_size])} rijen)\")\n",
    "                    except pyodbc.Error as e:\n",
    "                        conn.rollback()\n",
    "                        print(f\"  ❌ Fout in batch {i//batch_size + 1}: {str(e)}\")\n",
    "                \n",
    "            print(\"\\n⏳ Herinschakelen van FOREIGN KEY constraints...\")\n",
    "            cursor.execute(\"EXEC sp_MSforeachtable 'ALTER TABLE ? CHECK CONSTRAINT ALL'\")\n",
    "            conn.commit()\n",
    "            \n",
    "            print(\"\\n🎉 Upload voltooid voor alle tabellen!\")\n",
    "            cursor.close()\n",
    "            \n",
    "    except pyodbc.Error as e:\n",
    "        print(f\"❌ Databasefout: {e}\")\n",
    "\n",
    "upload_dataframes_to_sql(dfs_sourcedatamodel)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
